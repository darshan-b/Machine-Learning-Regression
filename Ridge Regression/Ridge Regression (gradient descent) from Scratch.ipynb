{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression (gradient descent) from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab # Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# House Sales Data from King County, Seatle, WA\n",
    "sales = graphlab.SFrame('kc_house_data.gl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(df,features,output):\n",
    "    df['constant']=1 # A new column for the constant/intercept term\n",
    "    features=['constant']+features # Column Names\n",
    "    sframe=df[features] # Copy all the data for the selected features in the new dataframe\n",
    "    smatrix=sframe.to_numpy() # Creating a numpy array for the features\n",
    "    output_array=np.array(df[output])# Array for the ouptut/target\n",
    "    return smatrix, output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_output(f, w):\n",
    "    # This fucntion will multiply the features by its weights\n",
    "    # So if there are D features, a weight is assigned to each feature\n",
    "    pred_out=np.dot(f,w)\n",
    "    return pred_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is the sum over the data points of the squared difference between an observed output and a predicted output, plus the L2 penalty term.\n",
    "```\n",
    "Cost(w)\n",
    "= SUM[ (prediction - output)^2 ]\n",
    "+ l2_penalty*(w[0]^2 + w[1]^2 + ... + w[k]^2).\n",
    "```\n",
    "\n",
    "We get\n",
    "```\n",
    "2*SUM[ error*[feature_i] ] + 2*l2_penalty*w[i].\n",
    "```\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself, plus `2*l2_penalty*w[i]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n",
    "    # If feature_is_constant is True, derivative is twice the dot product of errors and feature\n",
    "    if feature_is_constant==True:\n",
    "        derivative=2*np.dot(errors,feature)\n",
    "    # Otherwise, derivative is twice the dot product plus 2*l2_penalty*weight\n",
    "    else:\n",
    "        derivative=2*np.dot(errors,feature) + 2*l2_penalty*weight     \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your feature derivartive run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.65541667824e+13\n",
      "-22446749336.0\n"
     ]
    }
   ],
   "source": [
    "# Sample \n",
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
    "my_weights = np.array([1., 10.])\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "errors = test_predictions - example_output # prediction errors\n",
    "\n",
    "# For features it prints out the derivative\n",
    "print feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False)\n",
    "\n",
    "# For constant it prints out the derivative\n",
    "print feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of *increase* and therefore the negative gradient is the direction of *decrease* and we're trying to *minimize* a cost function. \n",
    "\n",
    "The amount by which we move in the negative gradient *direction*  is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. If no maximum number is supplied, the maximum should be set 100 by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=100):\n",
    "    print 'Starting gradient descent with l2_penalty = ' + str(l2_penalty)\n",
    "    my_weights = np.array(initial_weights) \n",
    "    iteration = 0  \n",
    "    while iteration<=max_iterations:\n",
    "        iteration += 1  \n",
    "        predictions=predict_output(feature_matrix,my_weights) # Predict the outcomes\n",
    "        errors=predictions-output # Calculate the error\n",
    "        for i in xrange(len(my_weights)):\n",
    "            if i==0:\n",
    "                derivative=2*np.dot(errors,feature_matrix[:,i]) # The partial derivative\n",
    "            else:\n",
    "                derivative=2*np.dot(errors,feature_matrix[:,i])+2*l2_penalty*my_weights[i] # The partial derivative\n",
    "            my_weights[i]-=step_size*np.sum(derivative)  # Update the weights\n",
    "    print 'Done with gradient descent at iteration ', iteration\n",
    "    print 'Learned weights = ', str(my_weights)\n",
    "    return my_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing effect of L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 penalty gets its name because it causes weights to have small L2 norms than otherwise. Let's see how large weights get penalized. Let us consider a simple model with 1 feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the dataset into training set and test set. Make sure to use `seed=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0) # Splitting the data\n",
    "\n",
    "# Simple features \n",
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "(simple_test_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the parameters for our optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ -1.63113501e-01   2.63024369e+02]\n",
      "Co-efficient of sqft_living is:  263.024368907\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters\n",
    "initial_weights = np.array([0., 0.])\n",
    "step_size = 1e-12 # Step size: constant in this case, we can also have decreasing step size, to avoid overshooting\n",
    "max_iterations=1000 \n",
    "\n",
    "#Calling the ridge regression function\n",
    "simple_weights_0_penalty=ridge_regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, 0, 1000-1)\n",
    "print \"Co-efficient of sqft_living is: \",simple_weights_0_penalty[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's consider high regularization.  Set the `l2_penalty` to `1e11` and run your ridge regression algorithm to learn the weights of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 1e+11\n",
      "Done with gradient descent at iteration  1001\n",
      "Learned weights =  [   9.77704732  124.57217379]\n",
      "Co-efficient of sqft_living is:  124.572173792\n"
     ]
    }
   ],
   "source": [
    "#Initial parameters with high regularization i.e. high penalty causing high weights to be highly penalized\n",
    "l2=1e11\n",
    "simple_weights_high_penalty=ridge_regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, l2, 1000)\n",
    "print \"Co-efficient of sqft_living is: \",simple_weights_high_penalty[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will plot the two learned models.  (The blue line is for the model with no regularization and the red line is for the one with high regularization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x218ecb70>,\n",
       " <matplotlib.lines.Line2D at 0x218ecd30>,\n",
       " <matplotlib.lines.Line2D at 0x218ece48>,\n",
       " <matplotlib.lines.Line2D at 0x218fb898>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEACAYAAABoJ6s/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqoV73wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x217f32e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_0_penalty),'b-',\n",
    "          simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_high_penalty),'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of model with high regularizaton is: 6.946421e+14\n",
      "The RSS of model with no regularizaton is: 2.757236e+14\n"
     ]
    }
   ],
   "source": [
    "# Calculate high regulariztaion\n",
    "a=test_data['sqft_living']\n",
    "\n",
    "#Price= constant + weight for sqft_living * data[sqft_living]\n",
    "pred=a*simple_weights_high_penalty[1]+simple_weights_high_penalty[0]\n",
    "print \"The RSS of model with high regularizaton is:\",'%3e' %np.sum(np.square(pred-test_data['price']))\n",
    "\n",
    "\n",
    "pred=a*simple_weights_0_penalty[1]+simple_weights_0_penalty[0]\n",
    "print \"The RSS of model with no regularizaton is:\",'%3e' %np.sum(np.square(pred-test_data['price']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a multiple regression with L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider a model with 2 features: `['sqft_living', 'sqft_living15']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to re-inialize the weights, since we have one extra parameter. Let us also set the step size and maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0.0\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [  -0.35743482  243.0541689    22.41481594]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters\n",
    "initial_weights = np.array([0.0,0.0,0.0])\n",
    "step_size = 1e-12\n",
    "max_iterations = 1000\n",
    "\n",
    "# With no regularization\n",
    "l2_penalty=0.0\n",
    "multiple_weights_0_penalty=ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, 1000-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 1e+11\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [  6.7429658   91.48927361  78.43658768]\n"
     ]
    }
   ],
   "source": [
    "#With high regularization \n",
    "l2_penalty=0.0\n",
    "multiple_weights_high_penalty=ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, 1e11, 1000-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSS of model with high regularizaton is: 2.740676e+14\n"
     ]
    }
   ],
   "source": [
    "a=test_data['sqft_living','sqft_living15','price']\n",
    "\n",
    "# Predicting the price of the houses based on the weights calaculated using the weights\n",
    "# Price= constant + weight for sqft_living * data[sqft_living] +weights or sqft_living15 * data[sqft_living15]\n",
    "pred_low=a['sqft_living']*multiple_weights_0_penalty[1]+a['sqft_living15']*multiple_weights_0_penalty[2]+multiple_weights_0_penalty[0]\n",
    "\n",
    "print \"The RSS of model with high regularizaton is:\",'%3e' %np.sum(np.square(pred_low-test_data['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_high=a['sqft_living']*multiple_weights_high_penalty[1]+a['sqft_living15']*multiple_weights_high_penalty[2]+multiple_weights_high_penalty[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual price of House 1 in test data is: [310000.0]\n",
      "Predicted price of House 1 in test data using multiple features and no regularization is: 387465.476465\n",
      "Predicted price of House 1 in test data using multiple features and high regularization is: 270453.530305\n"
     ]
    }
   ],
   "source": [
    "print \"Actual price of House 1 in test data is:\",test_data.head(1)['price']\n",
    "print \"Predicted price of House 1 in test data using multiple features and no regularization is:\", pred_low[0]\n",
    "print \"Predicted price of House 1 in test data using multiple features and high regularization is:\", pred_high[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
